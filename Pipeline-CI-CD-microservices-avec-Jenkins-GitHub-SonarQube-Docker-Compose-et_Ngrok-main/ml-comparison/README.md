# Comparaison d'Algorithmes ML avec M√©triques RPU

Ce module permet de comparer diff√©rents algorithmes de machine learning avec calcul d√©taill√© des m√©triques **RPU (Recall, Precision, Uplift)** et g√©n√©ration de **matrices de confusion**.

## üìã Fonctionnalit√©s

- **7 Algorithmes de Classification** :
  - Random Forest
  - SVM (Support Vector Machine)
  - Logistic Regression
  - Naive Bayes
  - K-Nearest Neighbors (KNN)
  - Decision Tree
  - Gradient Boosting

- **M√©triques Calcul√©es** :
  - **Recall** (Sensibilit√©)
  - **Precision** (Pr√©cision)
  - **Uplift** (Am√©lioration par rapport au baseline)
  - Accuracy
  - F1-Score
  - AUC-ROC (pour classification binaire)
  - Cross-validation scores

- **Visualisations** :
  - Matrices de confusion pour tous les algorithmes
  - Comparaison graphique des m√©triques
  - Export des r√©sultats en JSON et CSV

## üöÄ Installation

### Pr√©requis

- Python 3.8+
- pip

### Installation des d√©pendances

```bash
cd ml-comparison
pip install -r requirements.txt
```

## üìñ Utilisation

### 1. Utilisation en ligne de commande

#### Avec donn√©es d'exemple (dataset Breast Cancer)

```bash
python run_comparison.py
```

#### Avec vos propres donn√©es CSV

```bash
python run_comparison.py --data votre_fichier.csv --target nom_colonne_cible --output mes_resultats
```

**Param√®tres disponibles** :
- `--data` : Chemin vers un fichier CSV
- `--target` : Nom de la colonne cible (si non sp√©cifi√©, derni√®re colonne utilis√©e)
- `--output` : Pr√©fixe pour les fichiers de sortie (d√©faut: `results`)
- `--test-size` : Proportion des donn√©es de test (d√©faut: 0.2)

### 2. Utilisation en tant que module Python

```python
from algorithm_comparison import AlgorithmComparator, load_sample_data
import numpy as np

# Charger des donn√©es
X, y = load_sample_data()

# Initialiser le comparateur
comparator = AlgorithmComparator(random_state=42)
comparator.initialize_algorithms()

# Pr√©parer les donn√©es
comparator.prepare_data(X, y, test_size=0.2)

# Entra√Æner et √©valuer
results = comparator.train_and_evaluate_all()

# Obtenir le r√©sum√© comparatif
summary = comparator.get_comparison_summary()
print(summary)

# Obtenir le meilleur algorithme
best_name, best_result = comparator.get_best_algorithm()
print(f"Meilleur: {best_name}")

# Visualiser les matrices de confusion
comparator.plot_confusion_matrices(save_path='confusion_matrices.png')

# Visualiser la comparaison des m√©triques
comparator.plot_metrics_comparison(save_path='metrics_comparison.png')

# Exporter les r√©sultats
comparator.export_results('results.json')
```

### 3. Utilisation via API Flask

D√©marrer le serveur API :

```bash
python api.py
```

L'API sera accessible sur `http://localhost:5000`

#### Endpoints disponibles :

1. **POST /api/initialize**
   - Initialise le comparateur avec des donn√©es
   - Body (optionnel) : `{"X": [[...]], "y": [...]}`
   - Si non fourni, utilise des donn√©es d'exemple

2. **POST /api/train**
   - Entra√Æne et √©value tous les algorithmes
   - Retourne les r√©sultats complets

3. **GET /api/comparison**
   - Retourne le r√©sum√© comparatif de tous les algorithmes

4. **GET /api/best**
   - Retourne le meilleur algorithme et ses m√©triques

5. **GET /api/confusion-matrix/<algorithm>**
   - Retourne la matrice de confusion d'un algorithme sp√©cifique
   - Exemple : `/api/confusion-matrix/Random Forest`

6. **GET /api/rpu-metrics**
   - Retourne toutes les m√©triques RPU pour tous les algorithmes

#### Exemple d'utilisation de l'API :

```bash
# Initialiser
curl -X POST http://localhost:5000/api/initialize

# Entra√Æner
curl -X POST http://localhost:5000/api/train

# Obtenir la comparaison
curl http://localhost:5000/api/comparison

# Obtenir le meilleur algorithme
curl http://localhost:5000/api/best

# Obtenir les m√©triques RPU
curl http://localhost:5000/api/rpu-metrics
```

## üìä M√©triques RPU Expliqu√©es

### Recall (Rappel / Sensibilit√©)
Mesure la capacit√© du mod√®le √† identifier tous les cas positifs r√©els.

```
Recall = TP / (TP + FN)
```

### Precision (Pr√©cision)
Mesure la proportion de pr√©dictions positives qui sont correctes.

```
Precision = TP / (TP + FP)
```

### Uplift (Am√©lioration)
Mesure l'am√©lioration de l'accuracy par rapport √† un mod√®le baseline (pr√©diction majoritaire).

```
Uplift = (Accuracy - Baseline Accuracy) / Baseline Accuracy
```

### Matrice de Confusion
Tableau qui montre les pr√©dictions correctes et incorrectes :

```
                Pr√©dit
              Positif  N√©gatif
R√©el Positif    TP      FN
     N√©gatif     FP      TN
```

O√π :
- **TP** (True Positive) : Correctement pr√©dit comme positif
- **TN** (True Negative) : Correctement pr√©dit comme n√©gatif
- **FP** (False Positive) : Incorrectement pr√©dit comme positif
- **FN** (False Negative) : Incorrectement pr√©dit comme n√©gatif

## üìÅ Structure des Fichiers G√©n√©r√©s

Apr√®s ex√©cution, les fichiers suivants sont g√©n√©r√©s :

- `results_summary.csv` : R√©sum√© comparatif au format CSV
- `results_results.json` : R√©sultats d√©taill√©s au format JSON
- `results_confusion_matrices.png` : Visualisation des matrices de confusion
- `results_metrics_comparison.png` : Comparaison graphique des m√©triques

## üìà Exemple de R√©sultats

### R√©sum√© Comparatif

| Algorithm | Accuracy | Precision | Recall | F1-Score | Uplift | AUC-ROC |
|-----------|----------|-----------|--------|----------|--------|---------|
| Random Forest | 0.9649 | 0.9647 | 0.9649 | 0.9648 | 0.9298 | 0.9987 |
| Gradient Boosting | 0.9649 | 0.9647 | 0.9649 | 0.9648 | 0.9298 | 0.9987 |
| SVM | 0.9474 | 0.9471 | 0.9474 | 0.9473 | 0.8947 | 0.9965 |
| ... | ... | ... | ... | ... | ... | ... |

### Format JSON des R√©sultats

```json
{
  "Random Forest": {
    "metrics": {
      "Accuracy": 0.9649,
      "Precision": 0.9647,
      "Recall": 0.9649,
      "F1-Score": 0.9648,
      "Uplift": 0.9298,
      "AUC-ROC": 0.9987
    },
    "confusion_matrix": [[71, 2], [2, 39]],
    "cv_mean": 0.9649,
    "cv_std": 0.0123
  }
}
```

## üîß Personnalisation

### Ajouter un nouvel algorithme

Modifiez la m√©thode `initialize_algorithms()` dans `algorithm_comparison.py` :

```python
def initialize_algorithms(self):
    self.models = {
        # ... algorithmes existants ...
        'Nouvel Algorithme': VotreClassifier(
            param1=value1,
            param2=value2
        )
    }
```

### Modifier les m√©triques calcul√©es

Modifiez la m√©thode `calculate_rpu_metrics()` pour ajouter d'autres m√©triques.

## üêõ D√©pannage

### Erreur : "No module named 'sklearn'"
```bash
pip install -r requirements.txt
```

### Erreur lors de la g√©n√©ration des visualisations
Assurez-vous que matplotlib est correctement install√© et que vous avez les permissions d'√©criture.

### Probl√®me avec les donn√©es
- V√©rifiez que votre fichier CSV est bien format√©
- Assurez-vous que la colonne cible existe
- V√©rifiez qu'il n'y a pas de valeurs manquantes

## üìù Notes

- Les algorithmes sont entra√Æn√©s avec des param√®tres par d√©faut optimis√©s
- Pour de meilleures performances, ajustez les hyperparam√®tres selon vos donn√©es
- Le calcul de l'Uplift utilise un baseline simple (pr√©diction majoritaire)
- L'AUC-ROC n'est calcul√© que pour les probl√®mes de classification binaire

## üîó R√©f√©rences

- [scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Matrice de Confusion](https://en.wikipedia.org/wiki/Confusion_matrix)
- [M√©triques de Classification](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)

---

**Auteur** : √âquipe de d√©veloppement  
**Date** : 2024

